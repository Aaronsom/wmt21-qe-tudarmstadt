wandb_project: wmt21-qe # optional
do_train: True
output_dir: results/qe_da/testing
max_seq_len: 50
task: qe_da   # qe_da = task 1, qe_hter = task 2
report_to: all
#prompt: [original, translation]
train:
  train_batchsize: 4 # should evenly divide 7000 for multi pair training to work with the current hack
  eval_batchsize: 4 # must evenly divide 1000 for multi pair training to work with the current hack
  max_steps: 2500
  logging_steps: 10
  eval_steps: 250
  gradient_accumulation_steps: 4
  save_total_limit: 2
  amp: True
  epochs: 1
  pair: # [en, zh] # list of pairs or just a pair
    - [ en, de ]
    - [ en, zh ]
    - [ et, en ]
test:
  batchsize: 4
  pairs:
    - [en, de]
    - [en, zh]
    - [et, en]
#    - [ne, en] #missing
#   - [ro, en]
#   - [ru, en]
#   - [si, en]