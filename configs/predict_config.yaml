wandb_project: wmt21-qe # optional
do_train: False
model: xlm-roberta-large #bert-base-multilingual-cased #xlm-roberta-large
output_dir: results/qe_da/testing
adapter_path: results\qe-da\architecture\train_xlmrl_en_zh_bs_8_red_16qe_da_2021-06-07_07-52\best_checkpoint
max_seq_len: 50
task: qe_da   # qe_da = task 1, qe_hter = task 2
report_to: none
madx2: False
architecture: base
reduction_factor: 8
dropout: 0.1
no_lang: True
predict: True
#prompt: [original, translation]
train:
  train_batchsize: 4 # should evenly divide 7000 for multi pair training to work with the current hack
  eval_batchsize: 50 # must evenly divide 1000 for multi pair training to work with the current hack
  max_steps: 10
  logging_steps: 10
  eval_steps: 250
  gradient_accumulation_steps: 2
  save_total_limit: 2
  amp: True
  epochs: 1
  pair: [en, zh] # list of pairs or just a pair
    #- [en, de]
    #- [en, zh]
test:
  batchsize: 32
  pairs:
    - [en, cs]
    - [en, de]
    - [en, zh]
