wandb_project: wmt21-qe # optional
do_train: False
model: xlm-roberta-large #bert-base-multilingual-cased #xlm-roberta-large
output_dir: results/qe_da/testing
adapter_path:
   - G:\Meine Ablage\Uni\Seminar\results\qe-da\ensemble\train_xlmrl_all_base_r8_bs8_lr1-4_seed1qe_da_2021-06-09_05-49\best_checkpoint
   - G:\Meine Ablage\Uni\Seminar\results\qe-da\ensemble\train_xlmrl_all_base_r8_bs8_lr1-4_seed2qe_da_2021-06-09_07-33\best_checkpoint
   - G:\Meine Ablage\Uni\Seminar\results\qe-da\ensemble\train_xlmrl_all_base_r8_bs8_lr1-4_seed3qe_da_2021-06-09_09-18\best_checkpoint
   - G:\Meine Ablage\Uni\Seminar\results\qe-da\ensemble\train_xlmrl_all_base_r8_bs8_lr1-4_seed4qe_da_2021-06-09_11-06\best_checkpoint
max_seq_len: 50
task: qe_da   # qe_da = task 1, qe_hter = task 2
report_to: none
madx2: False
architecture: base
reduction_factor: 8
dropout: 0.1
no_lang: True
predict: True
#prompt: [original, translation]
train:
  train_batchsize: 4 # should evenly divide 7000 for multi pair training to work with the current hack
  eval_batchsize: 50 # must evenly divide 1000 for multi pair training to work with the current hack
  max_steps: 10
  logging_steps: 10
  eval_steps: 250
  gradient_accumulation_steps: 2
  save_total_limit: 2
  amp: True
  epochs: 1
  pair: [en, zh] # list of pairs or just a pair
    #- [en, de]
    #- [en, zh]
test:
  batchsize: 32
  pairs:
    #- [en, cs]
    - [ro, en]
    #- [en, de]
    #- [en, zh]
